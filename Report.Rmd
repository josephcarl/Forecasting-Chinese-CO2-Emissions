---
title: "Forecasting Chinese Carbon Dioxide Emissions"
author: "Joseph Carl"
date: "June 7, 2017"
output:
  html_notebook: default
  pdf_document: default
---

## Introduction

research question/motivation, description of topic, lit. review

Over the past several decades, China's economy has experienced explosive growth. While China's economic development has lifted millions out of poverty and increased its citizens' quality of life, it has also led to unintended consequences, including increased pollution levels. Pollution has been linked to many health issues, and carbon dioxide pollution is the main driver of global climate change. Additionally, as of 2007, China is now the largest contributor to global carbon emissions. Understanding how emissions have changed over time and forecasting emissions in future years is vital to understanding the trajectory that Chinese emissions are taking. Forecasts are also necessary to begin implementing policies that will decrease the country's emissions. The purpose of this analysis is to model Chinese carbon dioxide (CO~2~) emissions per capita over time. Specifically, I intend to use a vector autoregression (VAR) model to understand the relationship between emissions, GDP growth, and electricity consumption.


```{r setup, echo=FALSE, message=FALSE}
# Required libraries and functions
library(readxl)
library(tidyverse)
library(urca)
library(dynlm)
library(forecast)
library(stargazer)
source("Functions_ECON_5305.R")
source("data_cleaning_transformation.R")
```

## Data

description of data, data source(s), transformations to data, unit root tests, maybe some EDA

The data for this analysis was taken from Gapminder, an independent Swedish foundation dedicated to educating the public about global development and using statistics to promote a fact-based worldview. Though the data was downloaded from gapminder.org, it was compiled there from numerous sources. The CO~2~ emissions data are from the Carbon Dioxide Information Analysis Center at the U.S. Department of Energy. The two explanatory variables for the model are electricity consumption per capita and GDP per capita growth rates, as these may also explain a large portion of the variation in CO~2~ emissions per capita over time. The data on GDP growth rates and electricity consumption per capita are from the World Bank DataBank. The data for the model is annual data ranging from 1972-2010. 

## Methodology

research question, empirical methods (VAR), lag order selection, logic behind imposing the structure, how methodology will answer research question

### Unit Root Tests

Though the initial data comes in its original units (tonnes of CO~2~ per capita, GDP per capita percentage growth, and tonnes of oil equivalent per person), it is appropriate to test for a unit root on each of these variables. Because vector autoregression models are built from autoregressive properties, I test for a unit root to make sure the process being modeled is covariance stationary. Making sure that each variable is covariance stationary will also help with selecting the correct number of lags that should be included in the model. A quick look at each of the three variables suggests that annual CO~2~ emissions and electricity consumption do not have a constant mean over time and thus may not be covariance stationary, but GDP growth per capita might be stationary.

```{r, echo=FALSE, fig.height=2}
local({
  par(mfrow=c(1,3))
  ts.plot(China.co2, main="Annual CO2 Emissions", xlab = "Year", ylab = "Tonnes/Person")
  ts.plot(g.GDP, main="GDP Growth per Capita", xlab="Year", ylab = "% Change")
  ts.plot(ChinaElec, main="Annual Electric Consumption", xlab = "Year", ylab = "Kilowatt-Hours/Person")
})
```

To test for unit root, I use the augmented Dickey-Fuller test. The augmented Dickey Fuller tests the null hypothesis that the data is non-stationary against the alternate hypothesis that the data is stationary. The test must be run under three separate cases: that the data follows a random walk without drift or trend, a random walk with drift but no trend, or a random walk with drift and trend. The results of the augmented Dickey-Fuller tests are displayed for each variable are displayed in the following table.

```{r, echo=FALSE, results='asis'}
##### Unit Root Tests #####
# print("Annual CO2 Emissions")
adf.results(China.co2, max.lags = 1) %>% 
  knitr::kable(., format = "latex", caption = "Annual CO2 Emissions")
# print("Annual GDP per Capita Growth")
adf.results(g.GDP, max.lags = 1) %>% 
  knitr::kable(., format = "latex", caption = "Annual GDP per Capita Growth")
# print("Annual Electricity Consumption")
adf.results(ChinaElec, max.lags = 1) %>% 
  knitr::kable(., format = "latex", caption = "Annual Electricity Consumption per Capita")
```

As the results show, we fail to reject the null hypothesis of nonstationarity for annual carbon dioxide emissions and coal consumption. However, we are able to reject the null hypothesis that GDP per capita growth is nonstationary in favor of the alternate hypothesis that it is stationary. This makes sense because this variable is already a growth rate, and growth rates tend to be stationary. I then log-difference the CO~2~ emissions and electricity consumption data so that those transformed variables are the growth rates of the original variables, and rerun the augmented Dickey-Fuller test on the transformed data. Those results are presented below.

```{r, echo=FALSE, results='asis'}
# CO2 growth
# print("Change in Annual CO2 Emissions (%)")
adf.results(g.CO2, max.lags = 1) %>% 
  knitr::kable(., format = "latex", caption = "Change in Annual CO2 Emissions (\\%)")
# print("Change in Annual Electricity Consumption (%)")
adf.results(g.elec, max.lags = 1) %>% 
  knitr::kable(., format = "latex", caption = "Change in Annual Electricity Consumption (\\%)")

```

The results show that on the transformed data, we can now safely reject the null hypothesis that the growth rate of CO~2~ emissions is nonstationary in favor of the alternate hypothesis that they are stationary. However, the test results say we cannot reject the null hypothesis that the growth rate of electricity consumption is nonstationary. From looking at the test statistics, though, two are quite close to their critical values. If I run an augmented Dickey-Fuller test but use a significance level of 0.10 instead of 0.05, I then reject the null hypothesis in favor of the alternate stationarity hypothesis. Similarly, from plotting the the growth rate of electricity consumption, it appears to be approximately stationary (see below).

Because of these test results at the 0.10 level, the data appearing to be approximately stationary, and the augmented Dickey-Fuller test known for having a high error rate, I decide to keep the growth rate of electricity consumption in my model instead of differencing the data again and including that variable. The two transformed variables along with the GDP growth rate will be used for the VAR model.

As the following graph shows, both transformed variables appear much more stationary than their original versions appeared.

```{r, echo=FALSE}
local({
  par(mfrow=c(1,2))
  ts.plot(g.CO2, main=expression(paste(Delta, "Annual CO2 Emissions")), 
          xlab="Year", ylab="% Change")
  ts.plot(g.elec, main=expression(paste(Delta, "Annual Electricity Consumption")), 
          xlab="Year", ylab="% Change")
})
```

### Lag Order Selection

Knowing that the three variables for the model are now stationary, it is important to enter the correct number of lags into the vector autoregression model. I examine the autocorrelation function (ACF) and partial autocorrelation function (PACF) of each variable as a way to gauge which number of lags is appropriate. The number of statistically significant spikes in the ACF corresponds to the order of the underlying moving average (MA) process, and the number of statistically significant spikes in the PACF corresponds to the order of the underlying autoregressive (AR) process. Because vector autoregression only uses autoregressive variables, I am more interested in the AR order. The ACF and PACF of each series is given in the following graphs, where the blue dotted bands are the cutoff for statistical significance. Lags falling outside of the blue band are statistically different from zero, indicating they have explanatory power for their respective series.

```{r, echo=FALSE}
local({
  par(mfrow=c(3,2), mar=c(2,4,4,4)) # Adjust margins so graphs don't overlap
  Acf(g.CO2, lag.max = 10, main="ACF of CO2 Growth Rate")
  Pacf(g.CO2, lag.max = 10, main="PACF of CO2 Growth Rate")
  Acf(g.GDP, lag.max = 10, main="ACF of GDP Growth Rate")
  Pacf(g.GDP, lag.max = 10, main="PACF of GDP Growth Rate")
  Acf(g.elec, lag.max = 10, main="ACF of Electricity Growth Rate")
  Pacf(g.elec, lag.max = 10, main="PACF of Electricity Growth Rate")
})
```

For the CO~2~ growth rate, there is one statistically significant spike in both the ACF and PACF, suggesting that series follows an ARMA(1,1) process. The GDP growth rates have two statistically significant spikes in the PACF, and the first and sixth spikes in the ACF are statistically significant. This suggests the series could follow an ARMA(2,1) or ARMA(2,6) process. However, since I am more interested in the AR(2) piece for the VAR model, I do not explore that further. The electricity growth rate has one statistically significant spike in both the ACF and PACF, suggesting an ARMA(1,1) process. Looking at only the AR lags, the variables have AR processes of 1, 2, and 2, so it looks like a VAR(1) or VAR(2) system would be appropriate to estimate.

For comparison, I also run the `VARselect` command from the `vars` package on the data. This command runs a VAR(1) through VAR(8) model on the data, and selects the VAR process that has the lowest AIC or BIC. The results of that test are given below.

```{r, echo=FALSE}
# 3 variables: g.CO2 (CO2), g.GDP, g.elec
z <- cbind(g.CO2, g.elec, g.GDP) %>% na.omit

vars::VARselect(z, lag.max = 8, type = "trend")
```

Though all the models have AICs and BICs close to each other, the results show that the lowest AIC/BIC occur on the AR(1) model. Because of this, I estimate a VAR(1) model for this analysis. Even though individual variables may have an AR(2) process, it could be the case that the variables as a system follow a VAR(1) process. Additionally, considering that the data for this analysis is yearly, it makes sense that carbon dioxide emissions, GDP growth, and electricity consumption from two or more years ago are unlikely to have significant explanatory power for those variables today.

### The Reduced-Form VAR Model

I estimate the reduced form VAR model of the following system of equations:

$$\begin{bmatrix}
    \Delta CO_2\\
    \Delta GDP \\
    \Delta Elec\end{bmatrix} 
    =
    \begin{bmatrix}
    \delta_{10} \\
    \delta_{20} \\
    \delta_{30}\end{bmatrix} 
    +
    \begin{bmatrix}
    \delta_{11} + \delta_{12} + \delta_{13} \\
    \delta_{21} + \delta_{22} + \delta_{23}\\
    \delta_{31} + \delta_{32} + \delta_{33}\end{bmatrix} 
    \times
    \begin{bmatrix}
    \Delta CO_{2.t-1}\\
    \Delta GDP_{t-1} \\
    \Delta Elec_{t-1} \end{bmatrix}
    +
    \begin{bmatrix}
    \delta_{14} + \delta_{15} + \delta_{16} \\
    \delta_{24} + \delta_{25} + \delta_{26}\\
    \delta_{34} + \delta_{35} + \delta_{36}\end{bmatrix} 
    \times
    \begin{bmatrix}
    \Delta CO_{2.t-2}\\
    \Delta GDP_{t-2} \\
    \Delta Elec_{t-2} \end{bmatrix}$$

My goal is to use these equations to estimate the B matrix and calculate impulse response functions. Simply put, I am estimating three separate equations, each of which says that the change in CO~2~ emissions per capita, change in electricity consumption per capita, and GDP growth are functions of the previous year's change in CO~2~ emissions per capita, change in electricity consumption per capita, and GDP growth. However, the matrix algebra involved in recovering the parameters requires the assumption that some coefficients equal zero, or else the model cannot be solved. 

For the Cholesky decomposition of the sigma matrix, I select an upper Cholesky matrix. I choose the upper matrix because of the order the variables enter the model (CO~2~, electricity growth, and then GDP), and the theory behind how the variables interact. For example, the second row, first column says that carbon dioxide growth has no effect on GDP growth, but the nonzero element in the first row second column says that GDP growth affects CO~2~ emissions. This makes sense, because more economic output means more production in factories, more cars on the road, and increased energy consumption, all of which increase emissions. It is a much weaker argument that some may cut back their energy consumption in response to economic growth, hence the zero element. The third row has zeros in the first two columns, indicating that carbon dioxide emissions and GDP growth do not affect growth in electricity consumption. The first of these makes sense, as electricity demand increases fossil fuel consumption, not the other way around. As for the second argument, it is still plausible that increased GDP could increase electricity consumption, as consumers and businesses begin to consume and invest more energy in response to economic growth. However, the alternate proposal that electricity consumption causes GDP growth is stronger, as electricity consumption is directly a part of the GDP calculation.

Using this Cholesky decomposition, I calculate the impulse response functions for each variable to simulate the effect of a shock to each variable, and how it affects the system of equations over time.

### Logic Behind Imposing the Structure

## Results

results, tables, economic interpretation, forecast a few periods?

The estimated results of the reduced-form equations are provided in the following table. 

```{r, echo=FALSE, warning=FALSE, results='asis'}
# Estimate the models
eq1 <- dynlm(g.CO2 ~ L(g.CO2, 1) + L(g.elec, 1) + L(g.GDP, 1), data = z)
eq2 <- dynlm(g.elec ~ L(g.CO2, 1) + L(g.elec, 1) + L(g.GDP, 1), data = z)
eq3 <- dynlm(g.GDP ~ L(g.CO2, 1) + L(g.elec, 1) + L(g.GDP, 1), data = z)

# Extract coefficients to a matrix
coef.mat <- local({
  out <- rbind(coef(eq1), coef(eq2), coef(eq3))
  rownames(out) <- c("eq.CO2", "eq.elec", "eq.GDP")
  out
})
Dhat.0 <- coef.mat[,1, drop=FALSE]        # matrix of constants
Dhat.1 <- coef.mat[,2:4, drop=FALSE]      # matrix of coefficeints t-1 variables

# Get fitted residuals
ehat.CO2 <- resid(eq1)
ehat.elec <- resid(eq2)
ehat.GDP <- resid(eq3)

ehat <- rbind(ehat.CO2, ehat.elec, ehat.GDP)

sigma.ehat <- var(t(ehat))

# Cholesky decomposition
B <- chol(sigma.ehat)
  # Cholesky by default returns the upper matrix, which is what I want in this case

stargazer(eq1, eq2, eq3, type = "latex", header = F,
          covariate.labels = c("lag CO2", "lag Electricity", "lag GDP"),
          dep.var.labels = c("CO2", "Electricity", "GDP"))

```

The F-statistic of each model is statistically significant and each R^2^ value is above 0.2, indicating all three equations are useful and have a decent amount of explanatory power for only having two explanatory variables each. Interestingly, though, multiple variables lack statistical significance in the models. The first equation says that a 1% increase in carbon dioxide emissions in the previous year is associated with a 0.7% increase in carbon dioxide emissions in the current year, all else held constant. However, a 1% increase in GDP in the previous year has almost no effect on emissions this year, and a 1% increase in electricity consumption in the previous year is actually associated with a 0.39% decrease in emissions in the following year, though this value is not statistically different from zero. In the second equation for GDP growth, the only significant variable is the lag of GDP growth, suggesting that previous year's emissions and electricity consumption growth do not explain GDP in the current year. The coefficients for each of those variables are quite large, though, saying that a 1% increase in emissions last year is associated with a 16.4% increase in GDP this year, whereas a 1% increase in electricity consumption this year is actually associated with a 38.0% decrease in GDP this year. Both of these values have large standard errors, though, indicating they are not statistically different from zero. Additionally, because of electricity consumption and emissions being highly correlated, that could explain the reason for the unexpected results. In the final equation, the only statistically significant predictor of electricity consumption growth is actually emissions growth in the previous year. A 1% increase in emissions last year is associated with a 0.65% increase in electricity consumption this year, all else constant. Though emissions seem unlikely to cause electricity demand, it could be that because these two are correlated, they tend to move in the same direction even one time period apart.

### Recovered Structural Parameters

```{r, echo=FALSE}
# Recovered structural parameters
solve(B)  # B-inverse
A0 <- (-1)*(solve(B) - diag(3))   # A0 matrix
Gamma.0 <- solve(B) %*% Dhat.0 
Gamma.1 <- solve(B) %*% Dhat.1
rownames(Gamma.0) <- c("g.C02", "g.GDP", "g.elec")
rownames(Gamma.1) <- c("g.C02", "g.GDP", "g.elec")
# View the recovered structural parameters
Gamma.0; Gamma.1

```


### Impulse Response Functions

One advantage of a VAR system of equations is it allows modeling a "shock" to one variable and calculating its effect on the system over several periods. To model this, I calculate the effect of a one standard deviation shock to each of the three variables, where one standard deviation refers to the deviation in the residuals of each of the three equations. The results of the three shocks are provided in the following graphs. 


```{r, echo=FALSE}
# Need to reorder the variables for IRF command because I am using upper Cholesky but
  # this function uses lower Cholesky decomposition
var.reordered <- vars::VAR(z[,c("g.GDP", "g.elec", "g.CO2")], p=1, type="const")
```

```{r, echo=FALSE}
# Plot the 10-step ahead IRFs for each variable

# This code chunk modified from solution found online: 
# https://stackoverflow.com/questions/40189328/r-plotting-irf-manually
  # I used this code method because it was easier to customize than the "canned" plot.irf method in the vars package
set.seed(50)
par(mfrow=c(3,3), oma = c(0,0,2,0) + 0.1, mar = c(5,4,1,0) + 0.1)
for (i in 1:3){
  for (j in 1:3){
    var_plot=vars::irf(var.reordered, impulse =  paste(colnames(var.reordered$y)[i]), 
                     response=paste(colnames(var.reordered$y)[j]), n.ahead = 10, ortho=TRUE, 
                     boot=TRUE, runs=1000, ci=0.95)
    plot(x=c(1:11), y=unlist(var_plot$Lower), type="l", lwd = 1.5, lty=2,col="red", 
        ylab=paste(colnames(var.reordered$y)[j]), 
        xlab="t Periods Ahead",
        main=paste(var_plot$impulse, "->", colnames(var.reordered$y)[j], sep = " "), 
        ylim=range(c(unlist(var_plot$Lower),unlist(var_plot$Upper))) )
    lines(x=c(1:11),y=unlist(var_plot$Upper),type="l",lwd = 1.5, lty=2,col="red")
    lines(x=c(1:11),y=unlist(var_plot$irf),type="l", lwd = 2)
    abline(a = NULL, h = 0)
  }
}
```


Each of the three graphs graphs refer to the individual variable being shocked: CO~2~ emissions, Electricity consumption, and GDP growth. The three rows of each graph refer to the variable affected by the shock: GDP growth, electricity consumption, and CO~2~ emissions. For example, the graph shows that a one-time spike in CO~2~ emissions results in a roughly 0.5% increase in GDP growth in the year following the shock, and the growth then decays to zero by the third year after the shock. However, because the confidence bands for the 95% confidence interval include zero, we cannot say that this effect is statistically different from zero. In other words, it is possible that a shock to emissions could actually have no effect on GDP growth, and could possibly even have a negative effect. The same shock to emissions has statistically no effect on electricity consumption and even CO~2~ emissions  0.04% increase in CO~2~ emissions for that year, which then declines over time. By year four, only emissions are only 0.01% higher than they were relative to pre-shock levels. The same shock to emissions does not affect GDP growth rates or electricity consumption in the same period as the shock, but two periods later the same shock results in GDP growth and electricity consumption growth roughly 0.7% and 0.015% higher, respectively. Those shocks then die out over the following years. It is interesting that increased emissions would suggest GDP growth also rises. However, thinking about a shock to emissions that was not caused by GDP growth would include things like environmental accidents, such as oil spills or methane leaks that are unrelated to economic output. For this kind of emissions shock, it is plausible that GDP could actually grow after an environmental accident, as the government and businesses would have to spend more to clean it up.

The layout of the shocks echoes the structure of the upper Cholesky matrix mentioned above. As mentioned by my theory, CO~2~ emissions are likely to be affected by changes in CO~2~, electricity consumption, and GDP at time $t=0$, which is reflected in the first column of the graph, as the effect of all three shocks on emissions are nonzero at $t=0$. Similarly, the upper Cholesky matrix says that CO~2~ does not affect electricity and GDP at time $t=0$, and both of those shocks are zero at $t=0$ in the first row of graphs. Similarly, changes in electricity consumption are theorized not to affect GDP at time $t=0$, and the second row, third column graph shows that this shock is zero at $t=0$.

## Conclusion

and shortcomings of model

## Sources

Boden, T.A., G. Marland, and R.J. Andres. 2013. "Global, Regional, and National Fossil-Fuel CO2 Emissions." Carbon Dioxide Information Analysis Center, Oak Ridge National Laboratory, U.S. Department of Energy, Oak Ridge, Tenn., U.S.A. doi 10.3334/CDIAC/00001_V2013

"BP statistical review of world energy." British Petroleum.  http://www.bp.com/en/global/corporate/energy-economics/statistical-review-of-world-energy.html (2012).

"DataBank: World Development Indicators." The World Bank. http://databank.worldbank.org (2017).

Hlavac, Marek (2015). stargazer: Well-Formatted Regression and Summary Statistics Tables. R package version 5.2. http://CRAN.R-project.org/package=stargazer 
 
Rosling, Hans. "Gapminder." GapMinder Foundation http://www.gapminder.org 91 (2009).

